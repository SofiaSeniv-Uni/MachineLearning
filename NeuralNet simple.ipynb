{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../..')\n",
    "from metrics import evaluate_classification\n",
    "from plots import plot_decision_boundary, plot_data\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes, activ_funcs, normalize = True, learning_rate = 0.01, num_iter = 30000, eps = 10**-2, beta = 0.999):\n",
    "        self.layer_sizes = hidden_layer_sizes \n",
    "        self.layers_count = len(self.layer_sizes) + 1\n",
    "        self.activ_funcs = activ_funcs\n",
    "        self.normalize = normalize \n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iter = num_iter\n",
    "        self.eps = eps\n",
    "        self.beta = beta\n",
    "        self.COST_APPEND_T = 1\n",
    "        \n",
    "    def __normalize(self, X, mean = None, std = None):\n",
    "        m = mean\n",
    "        if m is None:\n",
    "            m = np.array([np.mean(X, axis=1)]).T\n",
    "        s = std\n",
    "        if s is None:\n",
    "            s = np.array([np.std(X, axis=1)]).T\n",
    "        X_new = (X - m) / s\n",
    "        return X_new, m, s\n",
    "\n",
    "    def __sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def __tanh(self, Z):\n",
    "        return (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "\n",
    "    def __relu(self, Z):\n",
    "        A = np.maximum(0, Z)\n",
    "        return A\n",
    "    \n",
    "    def __sigmoid_derivative(self, Z):\n",
    "        return np.multiply(Z, 1 - Z)\n",
    "    \n",
    "    def __softmax(self, Z):       \n",
    "        ex = np.exp(Z)        \n",
    "        return ex / np.sum(ex, axis=0, keepdims = True)\n",
    "    \n",
    "    def __initialize_parameters(self):\n",
    "        self.parameters = {}\n",
    "        self.rmsprop = {}\n",
    "        n_i = self.layer_sizes\n",
    "        for i in range(1, self.layers_count + 1):\n",
    "            self.parameters[f\"W{i}\"] = np.random.randn(n_i[i], n_i[i - 1]) * np.sqrt(2/n_i[i - 1])\n",
    "            self.parameters[f\"b{i}\"] = np.zeros((n_i[i], 1))\n",
    "            self.rmsprop[f\"SdW{i}\"] = np.zeros((n_i[i], n_i[i - 1]))\n",
    "            self.rmsprop[f\"Sdb{i}\"] = np.zeros((n_i[i], 1))\n",
    "       \n",
    "    def __forward_propagation(self, X):\n",
    "        cache = {\"A0\" : X}\n",
    "        for i in range(1, self.layers_count + 1):\n",
    "            cache[f\"Z{i}\"] = np.dot(self.parameters[f\"W{i}\"], cache[f\"A{i - 1}\"]) + self.parameters[f\"b{i}\"]\n",
    "            cache[f\"A{i}\"] = self.__softmax(cache[f\"Z{i}\"]) if i == self.layers_count else self.__sigmoid(cache[f\"Z{i}\"])\n",
    "\n",
    "        return cache[f\"A{self.layers_count}\"], cache\n",
    "        \n",
    "    def compute_cost(self, A, Y):\n",
    "        m = Y.shape[1]\n",
    "        res = Y * np.log(A) + (1 - Y) * np.log(1 - A)\n",
    "        J = -(1 / m) * np.sum(res)\n",
    "        return J\n",
    "        \n",
    "    def __backward_propagation(self, X, Y, cache):\n",
    "        m = X.shape[1]\n",
    "        gradients = {}\n",
    "        for i in reversed(range(1, self.layers_count + 1)):\n",
    "            if i == self.layers_count:\n",
    "                gradients[f\"dZ{i}\"] = cache[f\"A{i}\"] - Y\n",
    "            else:\n",
    "                dAi = np.dot(self.parameters[f\"W{i + 1}\"].T, gradients[f\"dZ{i + 1}\"])\n",
    "                gradients[f\"dZ{i}\"] = np.multiply(dAi, self.__sigmoid_derivative(cache[f\"A{i}\"]))\n",
    "                \n",
    "            gradients[f\"dW{i}\"] = (1/m) * np.dot (gradients[f\"dZ{i}\"], cache[f\"A{i - 1}\"].T)  \n",
    "            gradients[f\"db{i}\"] = (1/m) * np.sum(gradients[f\"dZ{i}\"], axis = 1, keepdims = True)\n",
    "                \n",
    "        return gradients\n",
    "    \n",
    "    def __update_parameters(self, gradients):\n",
    "        # for i in range(1, self.layers_count + 1):\n",
    "        #     dWi = gradients[f\"dW{i}\"]\n",
    "        #     dbi = gradients[f\"db{i}\"]\n",
    "        #     self.parameters[f\"W{i}\"] -= self.learning_rate * dWi\n",
    "        #     self.parameters[f\"b{i}\"] -= self.learning_rate * dbi\n",
    "        for i in range(1, self.layers_count + 1):\n",
    "            self.rmsprop[f\"SdW{i}\"] = self.beta * self.rmsprop[f\"SdW{i}\"] + (1 - self.beta) * gradients[f\"dW{i}\"]**2\n",
    "            self.rmsprop[f\"Sdb{i}\"] = self.beta * self.rmsprop[f\"Sdb{i}\"] + (1 - self.beta) * gradients[f\"db{i}\"]**2\n",
    "        \n",
    "        for i in range(1, self.layers_count + 1):\n",
    "            self.parameters[f\"W{i}\"] -= self.learning_rate * gradients[f\"dW{i}\"] / (np.sqrt(self.rmsprop[f\"SdW{i}\"]) + 1e-8)\n",
    "            self.parameters[f\"b{i}\"] -= self.learning_rate * gradients[f\"db{i}\"] / (np.sqrt(self.rmsprop[f\"Sdb{i}\"]) + 1e-8)\n",
    "    \n",
    "    def fit(self, X_vert, Y_vert, print_cost = True):\n",
    "        \n",
    "        lb = LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False) \n",
    "        lb.fit(Y_vert)\n",
    "        X, Y = X_vert.T, lb.transform(Y_vert).T\n",
    "        \n",
    "        self.layer_sizes.insert(0, X.shape[0]) #Input layer\n",
    "        self.layer_sizes.append(Y.shape[0]) #Output layer\n",
    "            \n",
    "        if self.normalize: #Normalize\n",
    "            X, self.__mean, self.__std = self.__normalize(X)\n",
    "                \n",
    "        self.__initialize_parameters()\n",
    "        \n",
    "        costs = [] #Costs log\n",
    "        for i in range(self.num_iter):\n",
    "            A, cache = self.__forward_propagation(X)\n",
    "\n",
    "            cost = self.compute_cost(A, Y)\n",
    "\n",
    "            gradients = self.__backward_propagation(X, Y, cache)\n",
    "\n",
    "            self.__update_parameters(gradients)\n",
    "\n",
    "            if print_cost and i % 1000 == 0: \n",
    "                print(\"{}-th iteration: {}\".format(i, cost))\n",
    "\n",
    "            if i % self.COST_APPEND_T == 0: \n",
    "                costs.append(cost)\n",
    "            \n",
    "            if(i>=self.COST_APPEND_T*2):\n",
    "                if(abs(costs[-1] - costs[-2]) < self.eps):\n",
    "                    break\n",
    "\n",
    "        if print_cost:\n",
    "            plt.plot(costs)\n",
    "            plt.ylabel(\"Cost\")\n",
    "            plt.xlabel(f\"Iteration, *{self.COST_APPEND_T}\")\n",
    "            plt.show()\n",
    "\n",
    "    def __gradient(self, parameters, gradients, AL, X, Y, cost_function, eps=1e-7):       \n",
    "        theta_plus = np.copy(Y).astype(float)\n",
    "        theta_plus += eps\n",
    "        J_plus = cost_function(AL, theta_plus)\n",
    "\n",
    "        theta_minus = np.copy(Y).astype(float)\n",
    "        theta_minus -= eps\n",
    "        J_minus = cost_function(AL, theta_minus)\n",
    "\n",
    "        gradapprox = (J_plus - J_minus) / (2 * eps)\n",
    "\n",
    "        numerator = np.linalg.norm(gradients - gradapprox)\n",
    "        denominator = np.linalg.norm(gradients) + np.linalg.norm(gradapprox)\n",
    "        difference = numerator / denominator\n",
    "        \n",
    "        return difference\n",
    "    \n",
    "        \n",
    "    def predict_proba(self, X_vert):\n",
    "        X = X_vert.T\n",
    "        if self.normalize:\n",
    "            X, _, _ = self.__normalize(X, self.__mean, self.__std)    \n",
    "        \n",
    "        probabilities = self.__forward_propagation(X)[0]\n",
    "        return probabilities.T\n",
    "        \n",
    "    def predict(self, X_vert):\n",
    "        probs = self.predict_proba(X_vert)\n",
    "        results_bin = (probs == probs.max(axis=1)[:, None]).astype(int)\n",
    "        return results_bin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Симульовані дані 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples = 200, n_classes = 2, n_features = 2, \n",
    "                           n_informative=2, n_redundant=0, random_state = 42,\n",
    "                           flip_y=0.02, class_sep=0.8)\n",
    "plot_data(X, y)\n",
    "plt.show()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.reshape(-1,1), test_size=0.2, random_state=42)\n",
    "\n",
    "hidden_layers_1 = [X.shape[1], 20, 20, 20, 20, 20, 1]\n",
    "activ_funcs_1 = ['relu', 'relu', 'relu', 'relu', 'relu', 'sigmoid']\n",
    "\n",
    "nn = NeuralNet(hidden_layers_1, activ_funcs_1)\n",
    "nn.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = nn.predict(X_test)\n",
    "\n",
    "accuracy, report, confusion = evaluate_classification(y_test, y_test_pred, normalize = True, learning_rate = 0.01, num_iter = 30000, eps = 10**-2, beta = 0.999)\n",
    "print(f\"Accuracy: \\n{accuracy}\")\n",
    "print(f\"Report: \\n{report}\")\n",
    "print(f\"Confusion: \\n{confusion}\")\n",
    "plot_decision_boundary(nn, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Симульовані дані 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "\n",
    "plot_data(X, y)\n",
    "plt.show()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.reshape(-1,1), test_size=0.2, random_state=42)\n",
    "\n",
    "hidden_layers_1 = [X.shape[1], 10, 10, 5]\n",
    "activ_funcs_1 = ['relu', 'relu', 'relu', 'sigmoid']\n",
    "\n",
    "nn = NeuralNet(hidden_layers_1, activ_funcs_1, learning_rate = 0.5)\n",
    "nn.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = nn.predict(X_test)\n",
    "\n",
    "accuracy, report, confusion = evaluate_classification(y_test, y_test_pred)\n",
    "print(f\"Accuracy: \\n{accuracy}\")\n",
    "print(f\"Report: \\n{report}\")\n",
    "print(f\"Confusion: \\n{confusion}\")\n",
    "plot_decision_boundary(nn, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = [\"test1\", \"test2\"]\n",
    "target_column = \"passed\"\n",
    "df = pd.read_csv(\"tests.csv\")\n",
    "X, y = df[data_columns].values, df[target_column].values\n",
    "\n",
    "plot_data(X, y)\n",
    "plt.show()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.reshape(-1,1), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers_1 = [X.shape[1], 10, 10, 5]\n",
    "activ_funcs_1 = ['relu', 'relu', 'relu','sigmoid']\n",
    "\n",
    "nn = NeuralNet(hidden_layers_1, activ_funcs_1, learning_rate = 0.5)\n",
    "nn.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = nn.predict(X_test)\n",
    "\n",
    "accuracy, report, confusion = evaluate_classification(y_test, y_test_pred)\n",
    "print(f\"Accuracy: \\n{accuracy}\")\n",
    "print(f\"Report: \\n{report}\")\n",
    "print(f\"Confusion: \\n{confusion}\")\n",
    "plot_decision_boundary(nn, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зверніть увагу на границю прийняття рішення. Модель старається побудувати складну криву, що може свідчити про її перетренування. Порівняйте отримані результати з класом з sklearn. Спробуйте додати нові шари для нашого класу та порівняти результати тоді. Поекспериментуйте з гіперпараметрами для обох класів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes = (20,), max_iter = 10000)#, activation = 'logistic', solver = 'sgd', learning_rate_init = 0.01, learning_rate = 'constant')\n",
    "\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy, report, confusion = evaluate_classification(y_test, y_test_pred)\n",
    "print(f\"Accuracy: \\n{accuracy}\")\n",
    "print(f\"Report: \\n{report}\")\n",
    "print(f\"Confusion: \\n{confusion}\")\n",
    "plot_decision_boundary(clf, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
